{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IST 718: Big Data Analytics\n",
    "\n",
    "- Professor: Willard Williamson <wewillia@syr.edu>\n",
    "- Faculty Assistant: Yash Pasar <yspasar@syr.edu>\n",
    "## General instructions:\n",
    "\n",
    "- You are welcome to discuss the problems with your classmates but __you are not allowed to copy any part of your answers from your classmates.  Short code snippets are allowed from the internet.  Code from the class text books or class provided code can be copied in its entirety.__\n",
    "- There could be tests in some cells (i.e., `assert` and `np.testing.` statements). These tests (if present) are used to grade your answers. **However, the professor and FAs could use __additional__ test for your answer. Think about cases where your code should run even if it passess all the tests you see.**\n",
    "- Before submitting your work, remember to check for run time errors with the following procedure:\n",
    "`Kernel`$\\rightarrow$`Restart and Run All`.  All runtime errors will result in a minimum penalty of half off.\n",
    "- Data Bricks is the official class runtime environment so you should test your code on Data Bricks before submission.  If there is a runtime problem in the grading environment, we will try your code on Data Bricks before making a final grading decision.\n",
    "- All plots shall include a title, and axis labels.\n",
    "- Grading feedback cells are there for graders to provide feedback to students.  Don't change or remove grading feedback cells.\n",
    "- Don't add or remove files from your git repo.\n",
    "- Do not change file names in your repo.  This also means don't change the title of the ipython notebook.\n",
    "- You are free to add additional code cells around the cells marked `your code here`.\n",
    "- Students may use toPandas() to print the head of data frames.\n",
    "- __Only use spark, spark machine learning, spark data frames, RDD's, and map reduce to solve all problems unless instructed otherwise.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements here\n",
    "# create spark and sparkcontext objects\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import isnan, isnull, when, count, col\n",
    "from pyspark.sql import functions as fn\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.ml import feature\n",
    "# Functionality for regression\n",
    "from pyspark.ml import regression\n",
    "# Funcionality for classification\n",
    "from pyspark.ml import classification\n",
    "# Object for creating sequences of transformations\n",
    "from pyspark.ml import Pipeline\n",
    "import seaborn as sns\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from math import exp, log10, fabs, atan, log\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not delete or change this cell\n",
    "\n",
    "import os\n",
    "\n",
    "# Define a function to determine if we are running on data bricks\n",
    "# Return true if running in the data bricks environment, false otherwise\n",
    "def is_databricks():\n",
    "    # get the databricks runtime version\n",
    "    db_env = os.getenv(\"DATABRICKS_RUNTIME_VERSION\")\n",
    "    \n",
    "    # if running on data bricks\n",
    "    if db_env != None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Define a function to read the data file.  The full path data file name is constructed\n",
    "# by checking runtime environment variables to determine if the runtime environment is \n",
    "# databricks, or a student's personal computer.  The full path file name is then\n",
    "# constructed based on the runtime env.\n",
    "# \n",
    "# Params\n",
    "#   data_file_name: The base name of the data file to load\n",
    "# \n",
    "# Returns the full path file name based on the runtime env\n",
    "#\n",
    "def get_training_filename(data_file_name):    \n",
    "    # if running on data bricks\n",
    "    if is_databricks():\n",
    "        # build the full path file name assuming data brick env\n",
    "        full_path_name = \"/FileStore/tables/%s\" % data_file_name\n",
    "    # else the data is assumed to be in the same dir as this notebook\n",
    "    else:\n",
    "        # Assume the student is running on their own computer and load the data\n",
    "        # file from the same dir as this notebook\n",
    "        full_path_name = data_file_name\n",
    "    \n",
    "    # return the full path file name to the caller\n",
    "    return full_path_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "Using the get_training_filename function defined in the cell above, read the sms_spam.csv file into a spark dataframe named spam_df.  There should be no empty columns in spam_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "spam_df = spark.read.csv(get_training_filename('sms_spam.csv'), header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "Starting with spam_df, create a new dataframe named spam_df1.  Rename the spam_df type column to be named spam.  In the spam column, replace the string `spam` the with the integer 1 and the string `ham` with the integer 0.  Print the head and shape of spam_df1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   spam                                               text\n",
      "0     0  Go until jurong point, crazy.. Available only ...\n",
      "1     0                      Ok lar... Joking wif u oni...\n",
      "2     1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3     0  U dun say so early hor... U c already then say...\n",
      "4     0  Nah I don't think he goes to usf, he lives aro...\n",
      "Shape of spam_df1 is:  (5574, 2)\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "spam_df1 = spam_df.select(fn.when(col('type')=='spam',1).otherwise(0).alias('spam'),'text')\n",
    "print(spam_df1.toPandas().head())         ## Printing the head of spam_df1\n",
    "print(\"Shape of spam_df1 is: \",(spam_df1.count(), len(spam_df1.columns)))        ##Printing the shape of the spam_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|spam|                text|\n",
      "+----+--------------------+\n",
      "|   0|Go until jurong p...|\n",
      "|   0|Ok lar... Joking ...|\n",
      "|   1|Free entry in 2 a...|\n",
      "|   0|U dun say so earl...|\n",
      "|   0|Nah I don't think...|\n",
      "+----+--------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(spam_df1.limit(5).show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "Starting with spam_df1, create a new dataframe named spam_df2 with a new column named filtered_text by removing stop words from the text column in spam_df.  Print the head and shape of spam_df2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   spam                                               text  \\\n",
      "0     0  Go until jurong point, crazy.. Available only ...   \n",
      "1     0                      Ok lar... Joking wif u oni...   \n",
      "2     1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3     0  U dun say so early hor... U c already then say...   \n",
      "4     0  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                       filtered_text  \n",
      "0  [jurong, point, crazy, available, bugis, n, gr...  \n",
      "1                     [ok, lar, joking, wif, u, oni]  \n",
      "2  [free, entry, wkly, comp, win, fa, cup, final,...  \n",
      "3               [u, dun, say, early, hor, u, c, say]  \n",
      "4             [nah, don, t, think, goes, usf, lives]  \n",
      "Shape of spam_df2 is:  (5574, 3)\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import requests\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "stop_words = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words').text.split()\n",
    "tokenizer = RegexTokenizer().setGaps(False)\\\n",
    "  .setPattern(\"\\\\p{L}+\")\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"words\")\n",
    "sw_filter = StopWordsRemover()\\\n",
    "  .setStopWords(stop_words)\\\n",
    "  .setCaseSensitive(False)\\\n",
    "  .setInputCol(\"words\")\\\n",
    "  .setOutputCol(\"filtered_text\")\n",
    "\n",
    "sw_remove_pipeline = Pipeline(stages=[tokenizer,sw_filter]).fit(spam_df1)\n",
    "\n",
    "spam_df2 = sw_remove_pipeline.transform(spam_df1).select('spam','text','filtered_text')\n",
    "\n",
    "print(spam_df2.toPandas().head())         ## Printing the head of spam_df2\n",
    "print(\"Shape of spam_df2 is: \",(spam_df2.count(), len(spam_df2.columns)))        ##Printing the shape of the spam_df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+\n",
      "|spam|                text|       filtered_text|\n",
      "+----+--------------------+--------------------+\n",
      "|   0|Go until jurong p...|[jurong, point, c...|\n",
      "|   0|Ok lar... Joking ...|[ok, lar, joking,...|\n",
      "|   1|Free entry in 2 a...|[free, entry, wkl...|\n",
      "|   0|U dun say so earl...|[u, dun, say, ear...|\n",
      "|   0|Nah I don't think...|[nah, don, t, thi...|\n",
      "+----+--------------------+--------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(spam_df2.limit(5).show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "Create a new dataframe named spam_df3 starting with spam_df2.  Create a new column named tfidf by performing a term frequency / inverse document frequency transformation on the filtered_text column of spam_df2.<br>  \n",
    "\n",
    "- Print the head and shape of spam_df3.  \n",
    "- Print the top 10 most important words indicated by the TFIDF score.  \n",
    "- Print the 10 least important words as indicated by the TFIDF score.\n",
    "- Print the total number of columns in the TFIDF data in spam_df3\n",
    "- Print the number of rows in the TFIDF data in spam_df3\n",
    "- Based only on the number of rows and columns in the TFIDF data, do you expect the model to overfit.  Explain your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   spam                                               text  \\\n",
      "0     0  Go until jurong point, crazy.. Available only ...   \n",
      "1     0                      Ok lar... Joking wif u oni...   \n",
      "2     1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
      "3     0  U dun say so early hor... U c already then say...   \n",
      "4     0  Nah I don't think he goes to usf, he lives aro...   \n",
      "\n",
      "                                       filtered_text  \\\n",
      "0  [jurong, point, crazy, available, bugis, n, gr...   \n",
      "1                     [ok, lar, joking, wif, u, oni]   \n",
      "2  [free, entry, wkly, comp, win, fa, cup, final,...   \n",
      "3               [u, dun, say, early, hor, u, c, say]   \n",
      "4             [nah, don, t, think, goes, usf, lives]   \n",
      "\n",
      "                                               tfidf  \n",
      "0  (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "1  (1.8914559363553713, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "2  (0.0, 4.732932264526793, 2.6026000033672867, 0...  \n",
      "3  (3.7829118727107427, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "4  (0.0, 0.0, 2.6026000033672867, 0.0, 0.0, 0.0, ...  \n",
      "Shape of spam_df3 is:  (5574, 4)\n",
      "One of the 10 most important words:  jabo\n",
      "One of the 10 most important words:  nammanna\n",
      "One of the 10 most important words:  lesser\n",
      "One of the 10 most important words:  mns\n",
      "One of the 10 most important words:  coherently\n",
      "One of the 10 most important words:  anand\n",
      "One of the 10 most important words:  memorable\n",
      "One of the 10 most important words:  riley\n",
      "One of the 10 most important words:  companion\n",
      "One of the 10 most important words:  dolls\n",
      "One of the 10 least important words:  u\n",
      "One of the 10 least important words:  s\n",
      "One of the 10 least important words:  m\n",
      "One of the 10 least important words:  t\n",
      "One of the 10 least important words:  just\n",
      "One of the 10 least important words:  ur\n",
      "One of the 10 least important words:  ok\n",
      "One of the 10 least important words:  ll\n",
      "One of the 10 least important words:  know\n",
      "One of the 10 least important words:  gt\n",
      "Number of columns in TFIDF data:  7443\n",
      "Number of rows in TFIDF data:  5574\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# we will remove words that appear in 5 docs or less\n",
    "cv = CountVectorizer()\\\n",
    "  .setInputCol(\"filtered_text\")\\\n",
    "  .setOutputCol(\"tf\")\n",
    "\n",
    "from pyspark.ml.feature import IDF\n",
    "idf = IDF().\\\n",
    "    setInputCol('tf').\\\n",
    "    setOutputCol('tfidf')\n",
    "\n",
    "tfidf_pipeline = Pipeline(stages=[cv,idf]).fit(spam_df2)\n",
    "spam_df3 = tfidf_pipeline.transform(spam_df2).select('spam','text','filtered_text','tfidf')\n",
    "print(spam_df3.toPandas().head())        ## Printing the head of spam_df3\n",
    "print(\"Shape of spam_df3 is: \",(spam_df3.count(), len(spam_df3.columns)))        ##Printing the shape of the spam_df3\n",
    "tfidf_score = list(tfidf_pipeline.stages[1].idf)\n",
    "vocab = list(tfidf_pipeline.stages[0].vocabulary)\n",
    "for i in (sorted(range(len(tfidf_score)), key=lambda i: tfidf_score[i])[-10:]):\n",
    "    print(\"One of the 10 most important words: \",vocab[i])\n",
    "for i in (sorted(range(len(tfidf_score)), key=lambda i: tfidf_score[i])[0:10]):\n",
    "    print(\"One of the 10 least important words: \",vocab[i])\n",
    "print(\"Number of columns in TFIDF data: \",len(tfidf_pipeline.stages[1].idf))\n",
    "print(\"Number of rows in TFIDF data: \",spam_df3.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your model overfit explanation here: Based on the number of rows and columns, we can surely expect the model to overfit as the number of columns are more than the number of rows and excessive number of features will result in overfitting which can be reduced by performing elastic net regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "Create a pipeline named pipe1 capable of predicting ham or spam using logistic regression using spam_df3 as input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|spam|                text|       filtered_text|               tfidf|       rawPrediction|         probability|prediction|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|   0|Go until jurong p...|[jurong, point, c...|(7443,[15,26,52,6...|[46.4832509984271...|[1.0,6.4950214616...|       0.0|\n",
      "|   0|Ok lar... Joking ...|[ok, lar, joking,...|(7443,[0,8,216,32...|[37.0230287333609...|[1.0,8.3387877120...|       0.0|\n",
      "|   1|Free entry in 2 a...|[free, entry, wkl...|(7443,[1,2,9,21,2...|[-25.376984268883...|[9.52611378547952...|       1.0|\n",
      "|   0|U dun say so earl...|[u, dun, say, ear...|(7443,[0,50,75,13...|[43.4350408188821...|[1.0,1.3689937049...|       0.0|\n",
      "|   0|Nah I don't think...|[nah, don, t, thi...|(7443,[2,30,46,33...|[37.2489681189668...|[1.0,6.6524051231...|       0.0|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression().\\\n",
    "    setLabelCol('spam').\\\n",
    "    setFeaturesCol('tfidf').\\\n",
    "    setRegParam(0.0).\\\n",
    "    setMaxIter(100).\\\n",
    "    setElasticNetParam(0.)\n",
    "\n",
    "pipe1 = Pipeline(stages=[lr])\n",
    "logisticregression_df = pipe1.fit(spam_df3).transform(spam_df3)\n",
    "logisticregression_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "Fit pipe1 using a [CrossValidator](https://spark.apache.org/docs/2.2.0/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) object with the number of cross validation folds = 3.  Score the model using a [BinaryClassificationEvaluator](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html) using ROC AUC as the metric.  Name the cross validator object cv1 and the fitted cross validator object fitted_cv1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "#from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "#from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\n",
    "grid = ParamGridBuilder().build()\n",
    "evaluator = BinaryClassificationEvaluator(labelCol = 'spam',metricName = 'areaUnderROC')\n",
    "cv1 = CrossValidator(estimator=pipe1, estimatorParamMaps=grid, evaluator=evaluator, numFolds = 3, seed = 65)\n",
    "fitted_cv1 = cv1.fit(spam_df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "Print the cross validation AUC score from fitted_cv1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "evaluator.evaluate(fitted_cv1.transform(spam_df3), {evaluator.metricName: \"areaUnderROC\"})   ### AUC on spamdf3_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "Create a ROC scatter plot from fitted_pipe1 TPR/FPR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "import matplotlib.pyplot as plt\n",
    "fitted_pipe1 = lr.fit(spam_df3)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.scatter(fitted_pipe1.summary.roc.select('FPR').collect(),\n",
    "         fitted_pipe1.summary.roc.select('TPR').collect())\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC')\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8\n",
    "Create a new cross validator object named cv2 similar to cv1 but this time add a ParamGridBuilder.  Define a grid of elastic net regularization parameters. Fit cv2 and name the resulting fitted cross validator fitted_cv2.  The number of parameters in your grid should be limited such that it runs in a reasonable amount of time (around 5 to 10 minutes max).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "#from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "#from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator\n",
    "grid = ParamGridBuilder().\\\n",
    "    addGrid(lr.regParam, [0.005, 0.01, 0.02,0.03]).\\\n",
    "    addGrid(lr.elasticNetParam, [0.05, 0.1, 0.2,0.3]).\\\n",
    "    build()\n",
    "evaluator = BinaryClassificationEvaluator(labelCol = 'spam',metricName = 'areaUnderROC')\n",
    "cv2 = CrossValidator(estimator=pipe1, estimatorParamMaps=grid, evaluator=evaluator, numFolds = 3, seed = 65)\n",
    "fitted_cv2 = cv2.fit(spam_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1242"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Counting number of features with non zero coefficients\n",
    "np.count_nonzero(fitted_cv2.bestModel.stages[0].coefficients.toArray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9\n",
    "- Print the resulting AUC from fitted_cv2. \n",
    "- Print the best model's L1 and L2 regularization parameters\n",
    "- Analyze the L1 feature selection:\n",
    "    - Print the total number of features\n",
    "    - Print the number of features that L1 regularization eliminated\n",
    "    - If any features were eliminated, print a sample of 10 words that were eliminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC from fitted_cv2:  0.9999894613326589\n",
      "The lambda value of L1 L2 regularization:  0.01\n",
      "The alpha value of L1 L2 regularization:  0.05\n",
      "The total number of features are:  7443\n",
      "Number of features that L1 eliminated:  6201\n",
      "10 Words that were eliminated:  ['s', 't', 'm', 'just', 'like', 'day', 'time', 'love', 'want', 'n']\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "print(\"AUC from fitted_cv2: \",evaluator.evaluate(fitted_cv2.bestModel.transform(spam_df3), {evaluator.metricName: \"areaUnderROC\"}))\n",
    "print(\"The lambda value of L1 L2 regularization: \",fitted_cv2.bestModel.stages[0]._java_obj.getRegParam())\n",
    "print(\"The alpha value of L1 L2 regularization: \",fitted_cv2.bestModel.stages[0]._java_obj.getElasticNetParam())\n",
    "print(\"The total number of features are: \",len(fitted_cv2.bestModel.stages[0].coefficients.toArray()))\n",
    "print(\"Number of features that L1 eliminated: \", len((fitted_cv2.bestModel.stages[0].coefficients.toArray()))-np.count_nonzero(fitted_cv2.bestModel.stages[0].coefficients.toArray()))\n",
    "coeffs = fitted_cv2.bestModel.stages[0].coefficients.toArray()\n",
    "dict_words_coeffs = {}\n",
    "for i in range(len(coeffs)):\n",
    "    dict_words_coeffs[vocab[i]] = abs(coeffs[i])\n",
    "eliminated_words = []\n",
    "for i in dict_words_coeffs.items():\n",
    "    if i[1] == 0:\n",
    "        eliminated_words.append(i[0])\n",
    "print(\"Sample of 10 Words that were eliminated: \",eliminated_words[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10\n",
    "Analyze the best model weights in fitted_cv2.  Print the 10 words that contribute the most to predicting spam.  Print the 10 words that contribute the least to predicting spam.  Do the words make sense?  Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 WORDS THAT CONTRIBUTE THE MOST & their COEFFICIENTS:  [('teenager', 0.854823549388191), ('freephone', 0.8353846142674471), ('voicemail', 0.8171072301477244), ('ringtoneking', 0.786089309503098), ('wining', 0.756191666401203), ('tattoos', 0.7061974274138932), ('premium', 0.6926164558744172), ('tf', 0.6912715845735495), ('minmoremobsemspobox', 0.5872896878935332), ('stchoice', 0.565547086246528)]\n",
      "\n",
      "10 WORDS THAT CONTRIBUTE THE LEAST & their COEFFICIENTS:  [('satisfied', 2.1745201889077365e-05), ('maneesha', 2.1745201889077365e-05), ('toll', 2.1745201889077365e-05), ('algarve', 3.2539738346245155e-05), ('eyes', 7.747065100211463e-05), ('solve', 9.063002627466452e-05), ('watching', 0.00013319164790986688), ('important', 0.00020379047099947615), ('long', 0.00041705389498167896), ('app', 0.0004471775791209139)]\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "top_10_words = []\n",
    "bottom_10_words = []\n",
    "dict_words_coeffs_sort_desc = {k: v for k, v in sorted(dict_words_coeffs.items(), key=lambda item: item[1],reverse=True)}\n",
    "dict_words_coeffs_sort_asc = {k: v for k, v in sorted(dict_words_coeffs.items(), key=lambda item: item[1])}\n",
    "for i in list(dict_words_coeffs_sort_desc.items())[0:10]:\n",
    "    top_10_words.append((i[0],i[1]))\n",
    "print(\"10 WORDS THAT CONTRIBUTE THE MOST & their COEFFICIENTS: \",top_10_words)\n",
    "for i in list(dict_words_coeffs_sort_asc.items()):\n",
    "    if i[1] != 0:\n",
    "        bottom_10_words.append((i[0],i[1]))\n",
    "print(\"\\n10 WORDS THAT CONTRIBUTE THE LEAST & their COEFFICIENTS: \",bottom_10_words[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your comments here: These words make sense because, some of the top 10 words that contribute to predicting spam are 'minmoremobsemspobox','stchoice','tf','ringtoneking' etc. which are clearly spam words and there is no confusion or ambiguity as to whether emails containing these words are spam or not as these are words don't have any meaning and hence all these emails can be classified as SPAM with full certainity.\n",
    "\n",
    "On the other hand, the words like 'satisfied','long','eyes','app' etc. can be very ambiguous and we cannot tell with full certainity as to whether the emails containing these words are SPAM or not. These emails are ambiguous as they may or may not be SPAM emails. Hence these words are the ones that contribute the least in predicting whether the email is a SPAM email or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra Credit (5 pts)**  This question is optional.  If you choose to answer this question, you will earn 5 extra credit points.  If you choose not to answer this question, no points will be deducted from your score.  Solve the following equation for $c$ symbolically using the python sympy package.  Convert the solved symbolic solution to a latex format (this can be done with a pyton call), then populate the solution cell with the resulting latex code so that your solution shows up symbolically similar the equation below.\n",
    "\n",
    "$$c g - c h + e \\left(a + 1\\right)^{b} - \\frac{d \\left(\\left(a + 1\\right)^{b} - 1\\right)}{a} + \\frac{f \\left(\\left(a + 1\\right)^{b} - 1\\right)}{a} = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not delete or change this cell\n",
    "\n",
    "# if running on data bricks\n",
    "if is_databricks():\n",
    "    # install sympy\n",
    "    dbutils.library.installPyPI\n",
    "    dbutils.library.installPyPI('sympy')\n",
    "    print(dbutils.library.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\left[ \\frac{- a e \\left(a + 1\\right)^{b} + d \\left(a + 1\\right)^{b} - d - f \\left(a + 1\\right)^{b} + f}{a \\left(g - h\\right)}\\right]\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import sympy as sp\n",
    "#from sympy.interactive import printing\n",
    "#printing.init_printing(use_latex = True)\n",
    "from sympy.solvers import solve\n",
    "from sympy import Symbol\n",
    "c,g,h,e,a,b,d,f = sp.symbols('c g h e a b d f')\n",
    "print(sp.latex(sp.solve((c*g) - (c*h) + (e*(a+1)**b) - ((d*(((a+1)**b)-1))/a) + ((f*(((a+1)**b)-1))/a), c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your latex output here such that a human readable equation is displayed for grading\n",
    "\n",
    "$$ \\left[ \\frac{- a e \\left(a + 1\\right)^{b} + d \\left(a + 1\\right)^{b} - d - f \\left(a + 1\\right)^{b} + f}{a \\left(g - h\\right)}\\right] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading Feedback"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
